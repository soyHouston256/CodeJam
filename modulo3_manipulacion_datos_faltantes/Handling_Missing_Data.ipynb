{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soyHouston256/CodeJam/blob/master/modulo3_manipulacion_datos_faltantes/Handling_Missing_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://posgrado.utec.edu.pe/sites/default/files/2023-08/Testimonial-home-2.jpg\" alt=\"HTML5 Icon\" width=\"900\" height=\"250\" >\n"
      ],
      "metadata": {
        "id": "63Z52XtzaaqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Handling Missing Data**"
      ],
      "metadata": {
        "id": "nXzKdHNnalF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objetivos**\n",
        "\n",
        "- Aplicar distintas técnicas de imputación:\n",
        "\n",
        "  - Medidas de tendencia central (media, mediana, moda)\n",
        "\n",
        "  - Imputación por regresión\n",
        "\n",
        "  - KNN imputation\n",
        "\n",
        "  - XGBoost imputador\n",
        "\n",
        "  - Autoencoders\n",
        "\n",
        "  - MICE (Multiple Imputation by Chained Equations)\n",
        "\n",
        "- Entrenar un modelo base (RandomForest o XGBoost) para comparar el impacto de cada estrategia de imputación en términos de performance (accuracy, AUC, etc.)\n",
        "\n",
        "- Reflexionar sobre las implicancias de cada método para modelos analíticos."
      ],
      "metadata": {
        "id": "l3bhQt_IalxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**\n",
        "\n",
        "Loan Prediction - train.csv\n",
        "\n",
        "Puedes obtenerlo desde:\n",
        " https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii"
      ],
      "metadata": {
        "id": "Inh96ly5azO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Librerías necesarias"
      ],
      "metadata": {
        "id": "rvo1B-sea4nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy seaborn matplotlib scikit-learn xgboost fancyimpute tensorflow missingno\n"
      ],
      "metadata": {
        "id": "RPK6gi4ka6Xj",
        "collapsed": true,
        "outputId": "3be22e41-b901-4c26-fd42-abbb43be79f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Collecting fancyimpute\n",
            "  Downloading fancyimpute-0.7.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: missingno in /usr/local/lib/python3.11/dist-packages (0.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Collecting knnimpute>=0.1.0 (from fancyimpute)\n",
            "  Downloading knnimpute-0.1.0.tar.gz (8.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.11/dist-packages (from fancyimpute) (1.6.6)\n",
            "Requirement already satisfied: cvxopt in /usr/local/lib/python3.11/dist-packages (from fancyimpute) (1.3.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from fancyimpute) (8.3.5)\n",
            "Collecting nose (from fancyimpute)\n",
            "  Downloading nose-1.3.7-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from cvxpy->fancyimpute) (1.0.4)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from cvxpy->fancyimpute) (0.11.1)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.11/dist-packages (from cvxpy->fancyimpute) (3.2.7.post2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->fancyimpute) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->fancyimpute) (1.6.0)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.11/dist-packages (from clarabel>=0.5.0->cvxpy->fancyimpute) (1.17.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from osqp>=0.6.2->cvxpy->fancyimpute) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi->clarabel>=0.5.0->cvxpy->fancyimpute) (2.22)\n",
            "Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fancyimpute, knnimpute\n",
            "  Building wheel for fancyimpute (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fancyimpute: filename=fancyimpute-0.7.0-py3-none-any.whl size=29879 sha256=46f8811194f48e6c1ea66ee25f111e34b86472756347d3049bda67c4c0b9fa5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/f3/a1/f7f10b5ae2c2459398762a3fcf4ac18c325311c7e3163d5a15\n",
            "  Building wheel for knnimpute (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for knnimpute: filename=knnimpute-0.1.0-py3-none-any.whl size=11331 sha256=27da764f965fd86fc21df33d533839f8f7520c3537b86bd68f3f580557c0a5c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/e8/e0/79872972161e54486517ae507f94b2c7cea27fb7ef793bd415\n",
            "Successfully built fancyimpute knnimpute\n",
            "Installing collected packages: nose, knnimpute, fancyimpute\n",
            "Successfully installed fancyimpute-0.7.0 knnimpute-0.1.0 nose-1.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Carga y diagnóstico inicial"
      ],
      "metadata": {
        "id": "wOP8Iso2a8uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "df.drop(\"Loan_ID\", axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "8katKHaya_1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Medidas de tendencia central (media, mediana, moda)"
      ],
      "metadata": {
        "id": "LodYmLpGbCZl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80twajk3aaHM"
      },
      "outputs": [],
      "source": [
        "df_mean = df.copy()\n",
        "\n",
        "df_mean[\"LoanAmount\"].fillna(df_mean[\"LoanAmount\"].mean(), inplace=True)\n",
        "df_mean[\"Gender\"].fillna(df_mean[\"Gender\"].mode()[0], inplace=True)\n",
        "df_mean[\"Self_Employed\"].fillna(df_mean[\"Self_Employed\"].mode()[0], inplace=True)\n",
        "df_mean[\"Dependents\"].fillna(df_mean[\"Dependents\"].mode()[0], inplace=True)\n",
        "df_mean[\"Credit_History\"].fillna(df_mean[\"Credit_History\"].mode()[0], inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Imputación por regresión lineal"
      ],
      "metadata": {
        "id": "wQLWxMqmbK1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos el modelo de regresión lineal de scikit-learn\n"
      ],
      "metadata": {
        "id": "WW6LW43exv_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "LJi0IeN-xFG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos una copia del dataframe original para no modificarlo directamente\n"
      ],
      "metadata": {
        "id": "8NVRcys0xsSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_reg = df.copy()"
      ],
      "metadata": {
        "id": "7BZ1G8k0bODa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separamos las filas donde \"LoanAmount\" NO es nulo (estos datos los usaremos para entrenar el modelo)\n",
        "train = df_reg[df_reg[\"LoanAmount\"].notnull()]\n",
        "\n",
        "# Separamos las filas donde \"LoanAmount\" ES nulo (estos datos se imputarán)\n",
        "test = df_reg[df_reg[\"LoanAmount\"].isnull()]"
      ],
      "metadata": {
        "id": "rChdid-JxTL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seleccionamos como variables predictoras el ingreso del solicitante y del co-solicitante para el conjunto de entrenamiento\n"
      ],
      "metadata": {
        "id": "nnogvSmVxqFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train[[\"ApplicantIncome\", \"CoapplicantIncome\"]]\n",
        "\n",
        "# La variable objetivo a predecir es \"LoanAmount\"\n",
        "y_train = train[\"LoanAmount\"]\n",
        "\n",
        "# Para el conjunto de prueba (filas con \"LoanAmount\" nulo), también seleccionamos las variables predictoras\n",
        "X_test = test[[\"ApplicantIncome\", \"CoapplicantIncome\"]]\n"
      ],
      "metadata": {
        "id": "jaB0Z5fLxU40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instanciamos y entrenamos el modelo de regresión lineal"
      ],
      "metadata": {
        "id": "fGvwNdi5xiRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "36XKfdsLxemv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " el modelo entrenado para predecir los valores faltantes de \"LoanAmount\"\n"
      ],
      "metadata": {
        "id": "v2Q8Eqs2xk9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(X_test)"
      ],
      "metadata": {
        "id": "5aMTe4AWxfxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Asignamos los valores imputados (predichos) a las posiciones originales donde \"LoanAmount\" era nulo\n"
      ],
      "metadata": {
        "id": "QXaRHlRzxnHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_reg.loc[df_reg[\"LoanAmount\"].isnull(), \"LoanAmount\"] = preds\n"
      ],
      "metadata": {
        "id": "wfObMr8DxgpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Imputación KNN"
      ],
      "metadata": {
        "id": "YFnm0CuwbQi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos el imputador KNN de scikit-learn\n",
        "y LabelEncoder para convertir variables categóricas a numéricas\n",
        "\n"
      ],
      "metadata": {
        "id": "Phowr3P6y10D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "RCsES-Y3y0V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos una copia del dataframe original para no modificarlo directamente\n"
      ],
      "metadata": {
        "id": "PNQsTaJmzIDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_knn = df.copy()"
      ],
      "metadata": {
        "id": "IZtvpYFybUN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicamos LabelEncoder a todas las columnas del dataframe.\n",
        "# Esto convierte variables categóricas (strings) en valores numéricos.\n",
        "# Nota: KNNImputer solo funciona con variables numéricas.\n",
        "df_knn = df_knn.apply(LabelEncoder().fit_transform)"
      ],
      "metadata": {
        "id": "KFpby7TKzQdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos una instancia del imputador KNN\n",
        "# n_neighbors=30 indica que se usarán los 30 registros más cercanos (similares)\n",
        "knn_imputer = KNNImputer(n_neighbors=30)"
      ],
      "metadata": {
        "id": "fhCpWXn6zTUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicamos el imputador al dataframe y reconstruimos el DataFrame imputado\n",
        "# fit_transform encuentra los valores faltantes y los imputa usando los vecinos más cercanos\n",
        "df_knn_imputed = pd.DataFrame(knn_imputer.fit_transform(df_knn), columns=df_knn.columns)"
      ],
      "metadata": {
        "id": "kqEaA7tXzT1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Imputación con XGBoost (modelo para imputar)"
      ],
      "metadata": {
        "id": "6BS0jGpibWTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos XGBoost, un modelo de gradient boosting muy potente para tareas de regresión y clasificación\n",
        "import xgboost as xgb\n",
        "\n"
      ],
      "metadata": {
        "id": "UunQbsprzjHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacemos una copia del DataFrame original para no modificarlo directamente\n",
        "df_xgb = df.copy()"
      ],
      "metadata": {
        "id": "hDf0-ugmzj5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Definimos una función para imputar una columna con valores faltantes usando un modelo XGBoost Regressor\n",
        "def xgb_impute(df, target_col):\n",
        "    # 1. Separar el conjunto de entrenamiento (valores NO nulos) y prueba (valores nulos) para la variable objetivo\n",
        "    train = df[df[target_col].notnull()]\n",
        "    test = df[df[target_col].isnull()]\n",
        "\n",
        "    # 2. Seleccionar las variables predictoras: excluye la columna objetivo y las columnas completamente vacías\n",
        "    features = [col for col in df.columns if col != target_col and df[col].notnull().sum() > 0]\n",
        "\n",
        "    # 3. Elimina registros del entrenamiento que aún tienen missing en las variables predictoras seleccionadas\n",
        "    train = train.dropna(subset=features)\n",
        "\n",
        "    # 4. Convierte variables categóricas a dummies (One-Hot Encoding) para entrenamiento\n",
        "    X_train = pd.get_dummies(train[features])\n",
        "\n",
        "    # 5. Define la variable objetivo (lo que vamos a imputar)\n",
        "    y_train = train[target_col]\n",
        "\n",
        "    # 6. Convierte también el conjunto de prueba a dummies\n",
        "    X_test = pd.get_dummies(test[features])\n",
        "\n",
        "    # 7. Asegura que X_test tenga las mismas columnas que X_train (rellena columnas faltantes con 0)\n",
        "    X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "    # 8. Entrena el modelo XGBoost con 100 árboles\n",
        "    model = xgb.XGBRegressor(n_estimators=100)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 9. Predice los valores faltantes en la variable objetivo\n",
        "    preds = model.predict(X_test)\n",
        "\n",
        "    # 10. Asigna los valores imputados a las posiciones originales con missing\n",
        "    df.loc[df[target_col].isnull(), target_col] = preds\n",
        "\n",
        "    # 11. Devuelve el DataFrame imputado\n",
        "    return df\n",
        "\n",
        "# Aplicamos la función para imputar los valores faltantes en la variable \"LoanAmount\"\n",
        "df_xgb = xgb_impute(df_xgb, \"LoanAmount\")\n"
      ],
      "metadata": {
        "id": "bg-F6wCNba5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Imputación MICE (Multivariate Imputation)"
      ],
      "metadata": {
        "id": "Wu8e7VBcbc27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos IterativeImputer desde fancyimpute (implementa el algoritmo MICE)\n",
        "from fancyimpute import IterativeImputer\n"
      ],
      "metadata": {
        "id": "tUOgnm10zwqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacemos una copia del DataFrame original para no modificarlo directamente\n",
        "df_mice = df.copy()"
      ],
      "metadata": {
        "id": "7k6byNLKzxZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Aplicamos LabelEncoder a todas las columnas para convertir variables categóricas a numéricas\n",
        "# Esto es necesario porque MICE solo puede trabajar con variables numéricas\n",
        "df_mice = df_mice.apply(LabelEncoder().fit_transform)\n",
        "\n",
        "# Creamos una instancia del imputador MICE (IterativeImputer)\n",
        "# Este imputador usa regresiones iterativas entre variables para predecir valores faltantes\n",
        "mice = IterativeImputer()\n",
        "\n",
        "# Aplicamos el imputador a los datos\n",
        "# fit_transform entrena los modelos internamente y realiza la imputación\n",
        "df_mice_imputed = pd.DataFrame(mice.fit_transform(df_mice), columns=df_mice.columns)\n"
      ],
      "metadata": {
        "id": "Tf_DU7a4bfZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Autoencoders para imputación"
      ],
      "metadata": {
        "id": "rUCnwkphbhF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos TensorFlow y los componentes necesarios para construir una red neuronal (autoencoder)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Hacemos una copia del DataFrame original para no modificarlo directamente\n",
        "df_auto = df.copy()\n",
        "\n",
        "# Convertimos todas las variables categóricas a numéricas con LabelEncoder\n",
        "df_auto = df_auto.apply(LabelEncoder().fit_transform)\n",
        "\n",
        "# Imputamos valores faltantes inicialmente con la media de cada columna\n",
        "# Esto es necesario porque las redes neuronales no pueden recibir NaNs como entrada\n",
        "df_auto = df_auto.fillna(df_auto.mean())\n",
        "\n",
        "# Convertimos el DataFrame a un array de NumPy para entrenar la red\n",
        "X = df_auto.values\n"
      ],
      "metadata": {
        "id": "vt8RT8IBbiuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la arquitectura del Autoencoder (una red neuronal simétrica)\n",
        "model = Sequential([\n",
        "    Dense(32, activation='relu', input_shape=(X.shape[1],)),  # Capa de entrada → 32 neuronas\n",
        "    Dense(16, activation='relu'),                              # Capa oculta comprimida (bottleneck)\n",
        "    Dense(32, activation='relu'),                              # Capa de expansión (simétrica)\n",
        "    Dense(X.shape[1])                                          # Capa de salida (reconstrucción completa)\n",
        "])\n"
      ],
      "metadata": {
        "id": "-CExkHDV0ZJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilamos el modelo usando el optimizador Adam y la pérdida de error cuadrático medio (MSE)\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Entrenamos el modelo para que aprenda a reconstruir sus propias entradas\n",
        "# El autoencoder aprende patrones en los datos para luego usarlos en la imputación\n",
        "model.fit(X, X, epochs=100, batch_size=16, verbose=0)\n"
      ],
      "metadata": {
        "id": "sFiqNCRo0auL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usamos el modelo entrenado para predecir la versión reconstruida de los datos\n",
        "# Esta salida será utilizada como imputación final\n",
        "X_pred = model.predict(X)\n",
        "\n",
        "# Convertimos la salida a un DataFrame con los mismos nombres de columnas que el original\n",
        "df_auto_imputed = pd.DataFrame(X_pred, columns=df_auto.columns)\n"
      ],
      "metadata": {
        "id": "_PqMGgWr0cq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Evaluación del impacto predictivo\n",
        "\n",
        "Utilizaremos un modelo de clasificación (XGBClassifier) para predecir Loan_Status con los diferentes datasets imputados."
      ],
      "metadata": {
        "id": "LdSdwVFKbm0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos herramientas\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from xgboost import XGBClassifier\n"
      ],
      "metadata": {
        "id": "KAmNaCV7brsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos una función que evalúa un DataFrame ya imputado\n",
        "# Entrena un modelo para predecir \"Loan_Status\" y calcula métricas de desempeño\n",
        "def evaluate_model(df, name=\"\"):\n",
        "    df = df.copy()  # Hacemos copia para no alterar el original\n",
        "\n",
        "    # Eliminamos cualquier fila que aún tenga valores faltantes\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Separamos variables predictoras (X) de la variable objetivo (y)\n",
        "    X = df.drop(\"Loan_Status\", axis=1)\n",
        "    y = df[\"Loan_Status\"]\n",
        "\n",
        "    # Si la variable objetivo es categórica (tipo object), la codificamos numéricamente\n",
        "    if y.dtype == 'O':\n",
        "        y = LabelEncoder().fit_transform(y)\n",
        "\n",
        "    # Convertimos todas las variables categóricas (en X) a variables dummy (One-Hot Encoding)\n",
        "    X = pd.get_dummies(X)\n",
        "\n",
        "    # Dividimos los datos en entrenamiento (80%) y prueba (20%)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Creamos y entrenamos el clasificador XGBoost\n",
        "    # Desactivamos use_label_encoder y usamos logloss como métrica\n",
        "    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Generamos predicciones de clase y de probabilidad\n",
        "    preds = model.predict(X_test)\n",
        "    probas = model.predict_proba(X_test)[:,1]  # Probabilidad de clase positiva\n",
        "\n",
        "    # Mostramos métricas: Accuracy y AUC\n",
        "    print(f\"Evaluación con {name}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, preds):.4f}\")\n",
        "    print(f\"AUC: {roc_auc_score(y_test, probas):.4f}\\n\")\n"
      ],
      "metadata": {
        "id": "TyVgRBSG1BkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluamos el desempeño del modelo con diferentes versiones del dataset imputado\n",
        "evaluate_model(df_mean, \"Media/Moda\")               # Imputación simple\n",
        "evaluate_model(df_reg, \"Regresión\")                 # Imputación por regresión lineal\n",
        "evaluate_model(df_knn_imputed, \"KNN\")               # Imputación con K-Nearest Neighbors\n",
        "evaluate_model(df_xgb, \"XGBoost\")                   # Imputación supervisada con XGBoost\n",
        "evaluate_model(df_mice_imputed, \"MICE\")             # Imputación iterativa por MICE\n",
        "df_auto_imputed[\"Loan_Status\"] = df[\"Loan_Status\"]\n",
        "evaluate_model(df_auto_imputed, \"Autoencoder\")      # Imputación usando autoencoders\n"
      ],
      "metadata": {
        "id": "g1CvOCD11EO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preguntas para reflexión\n",
        "\n",
        "- ¿Qué técnica ofrece mejor balance entre simplicidad y precisión?\n",
        "\n",
        "- ¿Qué riesgos podría implicar usar una técnica muy compleja como Autoencoders o XGBoost para imputar?\n",
        "\n",
        "- ¿Por qué KNN puede ser sensible a la escala de los datos o a outliers?\n",
        "\n",
        "- ¿Cómo se podría incorporar la incertidumbre de la imputación en el modelo final?\n",
        "\n",
        "- ¿Qué tipo de datos (categóricos, numéricos, multivariados) favorecen el uso de MICE sobre otros métodos?"
      ],
      "metadata": {
        "id": "vDWnUyhrb44e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusión\n",
        "\n",
        "La imputación no es solo un paso técnico: es una decisión analítica que puede alterar el resultado del modelo. Evaluar diferentes métodos no solo mejora el desempeño, sino también la confianza en los modelos desarrollados. Entender cuándo usar cada técnica y su impacto es clave para un análisis responsable."
      ],
      "metadata": {
        "id": "QVfOuKovb-94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Gracias por completar este laboratorio!\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "cDjdWiDRcDSI"
      }
    }
  ]
}